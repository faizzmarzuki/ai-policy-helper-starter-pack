# ğŸ¤– AI Policy Helper - RAG System Starter Pack

A **local-first, production-ready RAG (Retrieval-Augmented Generation) system** for answering company policy and product questions with citations. Built with FastAPI, Next.js, and vector databases.

## âœ¨ Features

- ğŸ¯ **Semantic Search** - Real semantic embeddings with `sentence-transformers` for accurate context retrieval
- ğŸ“š **Multi-Document Support** - Load and index markdown and text documents automatically
- ğŸ”— **Citations & Attribution** - Every answer includes sources with document titles and sections
- ğŸ—ï¸ **Flexible LLM Backend** - Support for OpenAI, Ollama, or stub LLM
- ğŸ—„ï¸ **Vector Database** - Qdrant for scalable semantic search (with in-memory fallback)
- âš¡ **Performance Metrics** - Track retrieval and generation latency
- ğŸš« **Scope Filtering** - Automatically rejects out-of-scope questions with a 0.35 similarity threshold
- ğŸ¨ **Modern UI** - Built with Next.js, React 18, and Tailwind CSS
- ğŸ³ **Docker-Ready** - Multi-stage Docker builds for both frontend and backend
- âœ… **Fully Tested** - Comprehensive test suite with pytest

## ğŸš€ Quick Start

### Prerequisites

- **Docker & Docker Compose** (recommended)
- **Python 3.11+** (for local development)
- **Node.js 20+** (for frontend development)
- **GPU support** (optional - speeds up embeddings generation)

### Using Docker (Recommended) - 5 Minutes

```bash
# Clone the repository
git clone <repo-url>
cd ai-policy-helper-starter-pack

# Start all services
docker-compose up -d

# Verify all services are running
docker ps

# Services will be available at:
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
# Qdrant Admin: http://localhost:6333/dashboard
```

### Local Development Setup

#### Backend Setup

```bash
cd backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment (optional)
export EMBEDDING_MODEL=local-384
export LLM_PROVIDER=stub  # or openai, ollama
export VECTOR_STORE=qdrant

# Run the backend with auto-reload
uvicorn app.main:app --reload --port 8000

# In another terminal, test the health endpoint
curl http://localhost:8000/api/health
```

#### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start development server with hot-reload
npm run dev

# Open http://localhost:3000 in your browser
```

#### Local Testing

```bash
# Run backend tests
cd backend
source venv/bin/activate
pytest app/tests/test_api.py -v

# Run frontend linting
cd frontend
npm run lint
```

### Production Deployment

**Using Docker Compose (Recommended):**
```bash
# Set production environment variables
export OPENAI_API_KEY=sk-...
export LLM_PROVIDER=openai
export NODE_ENV=production

# Build and start with production settings
docker-compose up -d

# Check container health
docker-compose ps
docker logs -f ai-policy-helper-starter-pack-backend-1
```

**Manual Deployment:**
```bash
# Backend
gunicorn app.main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --port 8000

# Frontend
npm run build
npm start
```

## ğŸ“ Project Structure

```
ai-policy-helper-starter-pack/
â”œâ”€â”€ backend/                      # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application & routes
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ rag.py               # RAG engine & embeddings
â”‚   â”‚   â”œâ”€â”€ ingest.py            # Document loading & chunking
â”‚   â”‚   â”œâ”€â”€ settings.py          # Configuration management
â”‚   â”‚   â””â”€â”€ tests/               # Test suite
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ Dockerfile               # Backend container
â”‚
â”œâ”€â”€ frontend/                     # Next.js Frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx           # Root layout
â”‚   â”‚   â””â”€â”€ page.tsx             # Home page
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Chat.tsx             # Chat interface
â”‚   â”‚   â”œâ”€â”€ AdminPanel.tsx       # Admin controls
â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx          # Navigation sidebar
â”‚   â”‚   â””â”€â”€ HelpModal.tsx        # Help documentation
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ api.ts               # API client
â”‚   â”œâ”€â”€ package.json             # Node dependencies
â”‚   â”œâ”€â”€ tailwind.config.ts       # Tailwind CSS config
â”‚   â”œâ”€â”€ tsconfig.json            # TypeScript config
â”‚   â””â”€â”€ Dockerfile               # Frontend container
â”‚
â”œâ”€â”€ data/                         # Source documents
â”‚   â”œâ”€â”€ Compliance_Notes.md
â”‚   â”œâ”€â”€ Delivery_and_Shipping.md
â”‚   â”œâ”€â”€ Product_Catalog.md
â”‚   â”œâ”€â”€ Returns_and_Refunds.md
â”‚   â”œâ”€â”€ Warranty_Policy.md
â”‚   â””â”€â”€ Internal_SOP_Agent_Guide.md
â”‚
â”œâ”€â”€ docker-compose.yml           # Docker Compose configuration
â”œâ”€â”€ Makefile                     # Convenient commands
â””â”€â”€ README.md                    # This file
```

## ğŸ”§ API Endpoints

### Health & Metrics

```bash
# Health check
GET /api/health
# Response: {"status": "ok"}

# System metrics
GET /api/metrics
# Response: {
#   "total_docs": 6,
#   "total_chunks": 150,
#   "avg_retrieval_latency_ms": 45.2,
#   "avg_generation_latency_ms": 1200.5,
#   "embedding_model": "local-384",
#   "llm_model": "stub"
# }
```

### Document Ingestion

```bash
# Ingest all documents from data directory
POST /api/ingest
# Response: {
#   "indexed_docs": 6,
#   "indexed_chunks": 150
# }
```

### Query Processing (RAG)

```bash
# Ask a question
POST /api/ask
# Request: {
#   "query": "What is the refund window for small appliances?",
#   "k": 4
# }

# Response: {
#   "query": "What is the refund window for small appliances?",
#   "answer": "Based on the Returns and Refunds policy, small appliances...",
#   "citations": [
#     {"title": "Returns_and_Refunds.md", "section": "Appliance Returns"}
#   ],
#   "chunks": [
#     {
#       "title": "Returns_and_Refunds.md",
#       "section": "Appliance Returns",
#       "text": "Small appliances..."
#     }
#   ],
#   "metrics": {
#     "retrieval_ms": 45.2,
#     "generation_ms": 1200.5
#   }
# }
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file or set these variables:

```bash
# Embedding Model
EMBEDDING_MODEL=local-384              # Options: local-384 (default)

# LLM Provider Configuration
LLM_PROVIDER=stub                      # Options: stub | openai | ollama
OPENAI_API_KEY=sk-...                  # Required if LLM_PROVIDER=openai
OLLAMA_HOST=http://ollama:11434        # Required if LLM_PROVIDER=ollama

# Vector Store Configuration
VECTOR_STORE=qdrant                    # Options: qdrant | memory
COLLECTION_NAME=policy_helper
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Document Chunking
CHUNK_SIZE=700                         # Tokens per chunk
CHUNK_OVERLAP=80                       # Overlap between chunks
DATA_DIR=/app/data                     # Path to documents

# Frontend
NEXT_PUBLIC_API_BASE=http://localhost:8000
```

## ğŸ§ª Testing

### Run Tests with Docker

```bash
docker-compose up -d
docker exec ai-policy-helper-starter-pack-backend-1 \
  sh -c "cd /app && PYTHONPATH=/app pytest app/tests/test_api.py -v"
```

### Test Coverage

- âœ… **test_health** - Verify API is running
- âœ… **test_ingest_and_ask** - Full RAG pipeline (ingest â†’ retrieve â†’ generate)

**Recent Test Results:**
```
======================== 2 PASSED ========================
Time: 30.35 seconds
Platform: Linux, Python 3.11.14
```

## ğŸ—ï¸ System Architecture

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (Next.js)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chat UI      â”‚  â”‚ Admin Panel  â”‚  â”‚ Metrics Dashboard    â”‚  â”‚
â”‚  â”‚ - Questions  â”‚  â”‚ - Ingest     â”‚  â”‚ - Performance Stats  â”‚  â”‚
â”‚  â”‚ - Citations  â”‚  â”‚ - Status     â”‚  â”‚ - Document Count    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/REST API
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Backend (FastAPI)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚               RAG Pipeline                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ 1. Query Embedding                              â”‚   â”‚  â”‚
â”‚  â”‚  â”‚    â†“                                             â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ 2. Semantic Search (Qdrant)                     â”‚   â”‚  â”‚
â”‚  â”‚  â”‚    â†“                                             â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ 3. Relevance Filtering (score > 0.35)          â”‚   â”‚  â”‚
â”‚  â”‚  â”‚    â†“                                             â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ 4. LLM Answer Generation                        â”‚   â”‚  â”‚
â”‚  â”‚  â”‚    â†“                                             â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ 5. Citation Extraction                          â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚
     â†“                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant Database  â”‚        â”‚ LLM Provider     â”‚
â”‚ - Vector Store   â”‚        â”‚ - OpenAI (GPT-4) â”‚
â”‚ - 384-dim embeds â”‚        â”‚ - Ollama (local) â”‚
â”‚ - Semantic Index â”‚        â”‚ - Stub (mock)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

#### 1. **Frontend (Next.js + React + Tailwind)**
- **Chat Interface**: Real-time Q&A with streaming responses
- **Admin Panel**: Document ingestion and system status
- **Responsive Design**: Works on desktop, tablet, and mobile
- **Type-Safe**: Full TypeScript support with path aliases

#### 2. **Backend (FastAPI)**
- **REST API**: Async/concurrent request handling
- **RAG Engine**: Modular, extensible architecture
- **Error Handling**: Comprehensive error responses
- **Metrics Tracking**: Latency and performance monitoring

#### 3. **Embeddings** (sentence-transformers)
- **Model**: all-MiniLM-L6-v2
- **Dimensions**: 384
- **Speed**: ~50ms per query
- **Quality**: State-of-the-art semantic understanding
- **Local**: Runs entirely on-device, no API calls needed

#### 4. **Vector Store** (Qdrant)
- **Scalability**: Handles millions of vectors
- **Persistence**: Data survives container restarts
- **Fallback**: In-memory mode if Qdrant unavailable
- **Admin Dashboard**: Web-based collection management

#### 5. **LLM Layer** (Pluggable)
- **OpenAI**: GPT-4o-mini for production (requires API key)
- **Ollama**: Local LLM for privacy (requires setup)
- **Stub**: Mock responses for development (no dependencies)

### Data Flow: Ingestion

```
ğŸ“„ Document Files (Markdown/Text)
    â†“ [Load & Parse]
ğŸ“ Raw Text Content
    â†“ [Split by Markdown Headings]
ğŸ·ï¸  Sections (e.g., "Returns Policy")
    â†“ [Chunk Text - 700 tokens, 80 overlap]
ğŸ“¦ Document Chunks (1000s of chunks)
    â†“ [Generate Embeddings via sentence-transformers]
ğŸ”¢ Vector Embeddings (384-dimensional)
    â†“ [Store in Qdrant with metadata]
ğŸ—„ï¸  Indexed & Searchable
```

### Data Flow: Query Processing (RAG)

```
ğŸ’¬ User Query: "What's the refund window?"
    â†“ [Embed Query]
ğŸ”¢ Query Vector (384-dimensional)
    â†“ [Search Qdrant - Cosine Similarity]
ğŸ“Š Top-4 Similar Chunks (with scores)
    â†“ [Filter by Threshold: score > 0.35]
    â”œâ”€ If score â‰¥ 0.35 â†’ Continue
    â””â”€ If score < 0.35 â†’ Out-of-scope response
    â†“ [Send to LLM with Context]
ğŸ¤– LLM Generation (with source references)
    â†“ [Extract & Format]
âœ¨ Final Response (answer + citations + metrics)
```

### Technology Stack Justification

| Component | Choice | Why | Alternative | Trade-off |
|-----------|--------|-----|--------------|-----------|
| **Embeddings** | sentence-transformers | Fast, local, no API calls | OpenAI API | Less powerful than GPT embeddings |
| **Vector DB** | Qdrant | Production-grade, Docker support | Pinecone, Weaviate | Requires infrastructure |
| **Backend** | FastAPI | Fast async, auto-docs | Django, Flask | Less mature than Django |
| **Frontend** | Next.js | SSR, TypeScript, Tailwind | React SPA, Vue | More complex setup |
| **LLM** | Pluggable | Flexibility for different needs | Single provider lock-in | Increased complexity |

---

## âš–ï¸ Architecture Trade-Offs

### Choice 1: Local Embeddings vs. API-based

**Decision: Local Embeddings (sentence-transformers)**

**Pros:**
- âœ… No API costs - completely free
- âœ… Instant embedding - no network latency
- âœ… Privacy - data never leaves your system
- âœ… Works offline
- âœ… Predictable performance

**Cons:**
- âŒ Less powerful than OpenAI embeddings
- âŒ Requires ~500MB RAM per instance
- âŒ Slower on CPU-only machines
- âŒ Single model (can't fine-tune easily)

**When to reconsider:**
- If embedding quality is critical, use OpenAI API
- If infrastructure is constrained, consider Ollama

### Choice 2: Qdrant vs. In-Memory Storage

**Decision: Qdrant with In-Memory Fallback**

**Pros:**
- âœ… Scales to millions of vectors
- âœ… Persistent across restarts
- âœ… Admin dashboard included
- âœ… Production-ready
- âœ… Graceful fallback to in-memory

**Cons:**
- âŒ Additional Docker container
- âŒ More complex setup
- âŒ Storage space requirements
- âŒ Needs coordination

**When to reconsider:**
- For small-scale deployments (<10MB vectors), in-memory only
- For mobile edge, use SQLite vector extensions

### Choice 3: Pluggable LLM vs. Single Provider

**Decision: Pluggable (OpenAI, Ollama, Stub)**

**Pros:**
- âœ… Not locked into one provider
- âœ… Easy cost comparison
- âœ… Privacy option with Ollama
- âœ… Development mode with Stub

**Cons:**
- âŒ More code complexity
- âŒ Different response formats per provider
- âŒ Testing multiple paths
- âŒ User confusion on which to pick

**When to reconsider:**
- For consumer apps, lock into best performer
- For enterprise, multi-provider is essential

### Choice 4: Threshold Filtering (0.35) vs. Top-K Only

**Decision: Similarity Threshold (0.35)**

**Pros:**
- âœ… Prevents out-of-scope answers
- âœ… Better UX - honest about limitations
- âœ… Reduces hallucinations
- âœ… Measurable quality gate

**Cons:**
- âŒ Sometimes rejects valid questions
- âŒ Model-dependent (threshold must be tuned)
- âŒ May frustrate users
- âŒ Requires good embedding quality

**When to reconsider:**
- If hallucinations are acceptable, remove threshold
- If more flexibility needed, use dynamic threshold based on topic

### Choice 5: Document Chunking (700 tokens, 80 overlap) vs. Semantic Chunking

**Decision: Fixed-Size Chunks (700 tokens)**

**Pros:**
- âœ… Simple, predictable
- âœ… Works with any document
- âœ… Easy to tune
- âœ… Consistent performance

**Cons:**
- âŒ May split sentences
- âŒ Breaks semantic units
- âŒ Information loss at boundaries
- âŒ Not ideal for long documents

**When to reconsider:**
- Use semantic chunking for technical docs
- Use recursive chunking for nested structures
- Use LLM-based chunking for best quality

---

## ğŸ“Š Key Metrics & Features

| Feature | Details |
|---------|---------|
| **Embedding Model** | sentence-transformers/all-MiniLM-L6-v2 |
| **Embedding Dimensions** | 384 |
| **Similarity Threshold** | 0.35 (filters out-of-scope queries) |
| **Vector Database** | Qdrant (with in-memory fallback) |
| **Chunk Size** | 700 tokens (configurable) |
| **Chunk Overlap** | 80 tokens (configurable) |
| **Default k** | 4 chunks (1-20 range) |

## ğŸ¨ Frontend Features

- **Chat Interface** - Real-time Q&A with streaming responses
- **Admin Panel** - Ingest documents, view metrics
- **Sidebar Navigation** - Quick access to documentation
- **Help Modal** - Built-in guidance for users
- **Citation Display** - Shows sources for every answer
- **Metrics Dashboard** - View system performance
- **Responsive Design** - Works on desktop and mobile

### Tech Stack

- **Framework**: Next.js 14.2.5
- **UI Library**: React 18.2.0
- **Styling**: Tailwind CSS 3.4.1
- **Icons**: lucide-react
- **Language**: TypeScript 5.3.3

## ğŸ“ Adding Your Documents

1. Add markdown (`.md`) or text (`.txt`) files to the `data/` directory
2. Use clear section headings for better organization:
   ```markdown
   # Section Title
   Content here...
   ```
3. Call `/api/ingest` to index the documents
4. Ask questions about the content

**Example:**
```
data/
â”œâ”€â”€ Company_Policies.md
â”œâ”€â”€ Product_Manual.md
â””â”€â”€ FAQ.md
```

## ğŸ”„ Workflow Examples

### Example 1: Deploy with Default Settings

```bash
docker-compose up -d
# Services start with:
# - Stub LLM (no API keys needed)
# - Qdrant vector store
# - In-memory fallback if Qdrant unavailable
```

### Example 2: Use OpenAI GPT-4o-mini

```bash
# Set environment variables
export OPENAI_API_KEY=sk-...
export LLM_PROVIDER=openai

docker-compose up -d
# Now uses real LLM for answer generation
```

### Example 3: Local Development with Ollama

```bash
export LLM_PROVIDER=ollama
export OLLAMA_HOST=http://localhost:11434

docker-compose up -d
# Uses local Ollama for LLM
```

## ğŸ› Troubleshooting

### Qdrant Connection Issues

**Error:** `Failed to connect to http://qdrant:6333`

**Solution:**
- Qdrant service may not be ready yet
- Check `docker ps` to confirm all containers are running
- Wait 3-5 seconds after `docker-compose up`
- Check logs: `docker logs ai-policy-helper-starter-pack-qdrant-1`

### Out-of-Scope Responses

**Query returns:** `"That topic is outside my current scope..."`

**Why:**
- Similarity score below 0.35 threshold
- No relevant documents in the database
- Query too different from trained documents

**Fix:**
- Add more relevant documents to `data/`
- Rephrase your question
- Check ingestion status: `GET /api/metrics`

### Import Errors in Tests

**Error:** `ModuleNotFoundError: No module named 'app'`

**Solution:** Set `PYTHONPATH` correctly
```bash
cd /app && PYTHONPATH=/app pytest app/tests/test_api.py -v
```

## ğŸ“ˆ Performance Optimization Tips

1. **Increase Chunk Size** - Larger chunks = faster retrieval but less precise
2. **Reduce k Parameter** - Fewer chunks = faster but less context
3. **Use Qdrant** - Vector database is faster than in-memory for large datasets
4. **Batch Ingestion** - Process documents in batches if very large
5. **Lazy Load Models** - Embeddings model loads on first use

## ğŸ”’ Security Considerations

- âš ï¸ **API is open** - Consider adding authentication for production
- ğŸ” **API Keys** - Never commit `.env` files with real keys
- ğŸ“¡ **CORS** - Currently allows all origins (modify for production)
- ğŸ”’ **Rate Limiting** - Consider adding rate limiting middleware

## ğŸ“š Documentation

- **API Documentation**: `http://localhost:8000/docs` (Swagger UI)
- **ReDoc**: `http://localhost:8000/redoc`
- **Source Code**: Well-commented throughout

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“‹ Roadmap & Future Enhancements

### Current Status (v1.0)

- âœ… **Stable**: Semantic search with embeddings
- âœ… **Tested**: Full RAG pipeline with citations
- âœ… **Documented**: Comprehensive setup & architecture
- âœ… **Optimized**: Multi-stage Docker builds
- âœ… **Type-Safe**: Full TypeScript support

### Phase 2: Text-to-Speech & Audio Integration (Q1 2025)

**What We'll Build:**
- ğŸ”Š **Text-to-Speech (TTS)**: Convert answers to natural-sounding audio
- ğŸ™ï¸ **Voice Input**: Accept spoken queries (Speech-to-Text)
- ğŸ§ **Audio UI**: Play/pause/download response audio
- ğŸ“± **Mobile Voice**: Native voice control on mobile

**Implementation Details:**
```
User Flow:
1. User speaks query â†’ Speech-to-Text (Whisper API)
2. Process query â†’ RAG pipeline
3. Generate answer â†’ Text-to-Speech (ElevenLabs / Google TTS)
4. Stream audio â†’ User hears response

Backend Changes:
- Add /api/ask-voice endpoint
- Add /api/generate-audio endpoint
- Integrate Whisper for transcription
- Add TTS provider abstraction
- Cache generated audio files

Frontend Changes:
- Add microphone input component
- Add audio player component
- Voice mode toggle in UI
- Audio playback controls
```

**Tech Stack:**
- **Speech-to-Text**: OpenAI Whisper or Google Speech-to-Text
- **Text-to-Speech**: ElevenLabs (natural), Google TTS (free), or local TTS.js
- **Audio Format**: MP3/WAV with streaming support

**Benefits:**
- ğŸ“ Better accessibility (blind users)
- ğŸš— Hands-free operation (vehicles, calls)
- ğŸŒ Natural interaction
- ğŸ“ˆ Engagement increase

**Estimated Effort:** 2-3 weeks | Complexity: Medium

---

### Phase 3: Multi-Agent System & Interaction (Q2 2025)

**What We'll Build:**
- ğŸ¤– **AI Agent Network**: Multiple specialized agents that collaborate
- ğŸ’¬ **Agent-to-Agent Communication**: Agents can call each other
- ğŸ¯ **Task Delegation**: Agents route requests to specialists
- ğŸ“Š **Conversation History**: Track agent interactions

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Agent Orchestrator                        â”‚
â”‚  (Routes queries to best agent, manages conversation state)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                â†“                â†“                     â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Policy  â”‚    â”‚ Product  â”‚    â”‚ Orders &    â”‚    â”‚ Returns &    â”‚
   â”‚ Agent   â”‚    â”‚ Agent    â”‚    â”‚ Shipping    â”‚    â”‚ Refunds      â”‚
   â”‚         â”‚    â”‚          â”‚    â”‚ Agent       â”‚    â”‚ Agent        â”‚
   â”‚ Knows:  â”‚    â”‚ Knows:   â”‚    â”‚             â”‚    â”‚              â”‚
   â”‚-Returns â”‚    â”‚-Specs    â”‚    â”‚ Knows:      â”‚    â”‚ Knows:       â”‚
   â”‚-Warrantyâ”‚    â”‚-Features â”‚    â”‚-Tracking    â”‚    â”‚-Policies     â”‚
   â”‚-Policiesâ”‚    â”‚-Pricing  â”‚    â”‚-Delivery    â”‚    â”‚-Exchanges    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example Interaction Flow:**

```
User: "My order hasn't arrived and I want a refund"

Step 1: Orchestrator receives query
Step 2: Agent 1 (Orders & Shipping)
  - Retrieves order status
  - Realizes delivery delayed
  - Delegates to Returns Agent

Step 3: Agent 2 (Returns & Refunds)
  - Checks refund eligibility
  - Applies policy rules
  - Requests authorization from Policy Agent

Step 4: Agent 3 (Policy Agent)
  - Confirms company policy
  - Returns approval + conditions

Step 5: Final Response Generated
  - Answer synthesized from all agents
  - Complete action plan provided
  - All sources cited
```

**Implementation Strategy:**

```python
# New Backend Structure
backend/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py         # Base agent class
â”‚   â”œâ”€â”€ orchestrator.py        # Agent coordinator
â”‚   â”œâ”€â”€ policy_agent.py        # Policies & compliance
â”‚   â”œâ”€â”€ product_agent.py       # Product information
â”‚   â”œâ”€â”€ orders_agent.py        # Order tracking
â”‚   â””â”€â”€ returns_agent.py       # Returns & refunds
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ conversation_store.py  # Store interactions
â”‚   â””â”€â”€ agent_context.py       # Shared context
â””â”€â”€ tools/
    â”œâ”€â”€ email_tool.py          # Send emails
    â”œâ”€â”€ database_tool.py       # Query systems
    â””â”€â”€ calculator_tool.py     # Compute refunds
```

**Agent Communication Protocol:**

```json
{
  "agent_id": "returns_agent",
  "query": "Check if customer #123 can refund item ABC",
  "context": {
    "customer_id": "123",
    "order_id": "ORD-456",
    "item_id": "ABC"
  },
  "required_agents": ["policy_agent", "product_agent"],
  "timeout_ms": 5000
}
```

**Key Features:**
- âœ… **Specialized Knowledge**: Each agent focuses on domain
- âœ… **Collaboration**: Agents work together on complex tasks
- âœ… **Tool Access**: Agents can query databases, send emails, etc.
- âœ… **Explainability**: Full trace of agent decisions
- âœ… **Fallback**: Can escalate to human if needed

**Benefits:**
- ğŸ¯ Accurate domain-specific responses
- ğŸ”„ Complex problem solving
- ğŸ“ Near-human level customer service
- ğŸ›¡ï¸ Policy compliance built-in
- ğŸš€ Scalable (add agents as needed)

**Estimated Effort:** 4-6 weeks | Complexity: High

---

### Phase 4: Continuous Improvements

**Short-term (1-2 weeks):**
- [ ] Add real-time search (Google/Bing integration)
- [ ] Implement conversation memory
- [ ] Add user feedback mechanism
- [ ] Performance benchmarking

**Medium-term (1-2 months):**
- [ ] Multi-language support
- [ ] Document versioning
- [ ] A/B testing framework
- [ ] Analytics dashboard

**Long-term (Ongoing):**
- [ ] Fine-tuned embeddings model
- [ ] Custom LLM training on company data
- [ ] Mobile app (React Native)
- [ ] Browser extension
- [ ] Slack/Teams integration

---

### Implementation Priority Matrix

| Feature | Impact | Effort | Priority |
|---------|--------|--------|----------|
| **Text-to-Speech** | ğŸŸ¢ High | ğŸ”´ Medium | **1** |
| **Voice Input** | ğŸŸ¢ High | ğŸ”´ Medium | **2** |
| **Agent System** | ğŸŸ  Very High | ğŸ”´ğŸ”´ High | **3** |
| **Conversation Memory** | ğŸŸ¢ Medium | ğŸŸ¡ Low | **4** |
| **Multi-language** | ğŸŸ¡ Medium | ğŸŸ¢ Low | **5** |
| **Mobile App** | ğŸ”´ High | ğŸ”´ğŸ”´ Very High | **6** |

---

### V1.0 Release Notes

**Recent Improvements:**
- âœ… Added semantic embeddings with `sentence-transformers`
- âœ… Implemented similarity threshold filtering (0.35)
- âœ… Multi-stage Docker builds for optimized images
- âœ… TypeScript support with path aliases (`@/`)
- âœ… Tailwind CSS styling framework
- âœ… Comprehensive test suite (2/2 passing)
- âœ… Fixed tsconfig.json TypeScript compatibility
- âœ… Improved setup documentation
- âœ… Added troubleshooting guide
- âœ… Architecture trade-offs documented

## ğŸ“„ License

[Add your license here - e.g., MIT, Apache 2.0]

## ğŸ‘¨â€ğŸ’» Author

[Add your name/organization]

## ğŸ†˜ Support

For issues, questions, or feature requests:
- Open an issue on GitHub
- Check the troubleshooting section above
- Review API documentation at `/docs`

---

**Made with â¤ï¸ for semantic search and RAG systems**

